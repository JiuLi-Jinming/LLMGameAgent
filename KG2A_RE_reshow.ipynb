{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d354b9-3e71-4bde-a0fc-1d70e9b33f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jinming/PhD/2nd_Project_GameAgents_pycharm/third-packages/stanford-corenlp-4.5.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "os.environ[\"CORENLP_HOME\"] = \"/home/jinming/PhD/2nd_Project_GameAgents_pycharm/third-packages/stanford-corenlp-4.5.7\"\n",
    "print(os.getenv(\"CORENLP_HOME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fafd7-3947-4d45-bc94-e64c8d3515a8",
   "metadata": {},
   "source": [
    "**Dependency Parse**: This will show relations like nsubj, prep, pobj, amod, which will be crucial in forming triples.\n",
    "\n",
    "**Named Entities**: The NER output will show named entities (e.g., LOCATION, ORGANIZATION) that can be helpful for identifying subjects and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d310886e-ea56-4aea-8a53-3434417c3030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "from stanfordnlp.server import CoreNLPClient\n",
       "\n",
       "# The observation text\n",
       "observation_text = \"\"\"\n",
       "You are standing in an open field west of a white house, with a boarded front door. \n",
       "There is a small mailbox here.\n",
       "\"\"\"\n",
       "\n",
       "# Set up the CoreNLP client\n",
       "with CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse'], timeout=30000, memory='4G') as client:\n",
       "    \n",
       "    # Annotate the text\n",
       "    ann = client.annotate(observation_text)\n",
       "    \n",
       "    # Iterate through sentences\n",
       "    for sentence in ann.sentence:\n",
       "        # Print dependency parsing information\n",
       "        print(\"Dependency Parse:\")\n",
       "        for dep in sentence.basicDependencies.edge:\n",
       "            dep_relation = dep.dep\n",
       "            governor = sentence.token[dep.source - 1].word  # source token (index is 1-based)\n",
       "            dependent = sentence.token[dep.target - 1].word  # target token\n",
       "            print(f\"{governor} ({dep_relation}) -> {dependent}\")\n",
       "        \n",
       "        # Print Named Entities for entity extraction\n",
       "        print(\"\\nNamed Entities:\")\n",
       "        for token in sentence.token:\n",
       "            word = token.word\n",
       "            ner = token.ner\n",
       "            print(f\"{word} -> {ner}\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "# The observation text\n",
    "observation_text = \"\"\"\n",
    "You are standing in an open field west of a white house, with a boarded front door. \n",
    "There is a small mailbox here.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the CoreNLP client\n",
    "with CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse'], timeout=30000, memory='4G') as client:\n",
    "    \n",
    "    # Annotate the text\n",
    "    ann = client.annotate(observation_text)\n",
    "    \n",
    "    # Iterate through sentences\n",
    "    for sentence in ann.sentence:\n",
    "        # Print dependency parsing information\n",
    "        print(\"Dependency Parse:\")\n",
    "        for dep in sentence.basicDependencies.edge:\n",
    "            dep_relation = dep.dep\n",
    "            governor = sentence.token[dep.source - 1].word  # source token (index is 1-based)\n",
    "            dependent = sentence.token[dep.target - 1].word  # target token\n",
    "            print(f\"{governor} ({dep_relation}) -> {dependent}\")\n",
    "        \n",
    "        # Print Named Entities for entity extraction\n",
    "        print(\"\\nNamed Entities:\")\n",
    "        for token in sentence.token:\n",
    "            word = token.word\n",
    "            ner = token.ner\n",
    "            print(f\"{word} -> {ner}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e423fb86-f7df-4406-93d0-18bde60e5ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "from stanfordnlp.server import CoreNLPClient\n",
       "\n",
       "# The observation text\n",
       "observation_text = \"\"\"\n",
       "West of House\n",
       "You are standing in an open field west of a white house, with a boarded front door.There is a small mailbox here.\n",
       "\"\"\"\n",
       "\n",
       "# Start the CoreNLP Client\n",
       "with CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse'], timeout=30000, memory='4G') as client:\n",
       "    \n",
       "    # Annotate the text\n",
       "    ann = client.annotate(observation_text)\n",
       "\n",
       "    # Iterate through sentences and print the dependency parse\n",
       "    for sentence in ann.sentence:\n",
       "        print(\"\\nSentence:\", \" \".join([token.word for token in sentence.token]))\n",
       "        print(\"Dependency Parse:\")\n",
       "        for dep in sentence.basicDependencies.edge:\n",
       "            dep_relation = dep.dep\n",
       "            governor = sentence.token[dep.source - 1].word  # source token (index is 1-based)\n",
       "            dependent = sentence.token[dep.target - 1].word  # target token\n",
       "            print(f\"{governor} ({dep_relation}) -> {dependent}\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "# The observation text\n",
    "observation_text = \"\"\"\n",
    "West of House\n",
    "You are standing in an open field west of a white house, with a boarded front door.There is a small mailbox here.\n",
    "\"\"\"\n",
    "\n",
    "# Start the CoreNLP Client\n",
    "with CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse'], timeout=30000, memory='4G') as client:\n",
    "    \n",
    "    # Annotate the text\n",
    "    ann = client.annotate(observation_text)\n",
    "\n",
    "    # Iterate through sentences and print the dependency parse\n",
    "    for sentence in ann.sentence:\n",
    "        print(\"\\nSentence:\", \" \".join([token.word for token in sentence.token]))\n",
    "        print(\"Dependency Parse:\")\n",
    "        for dep in sentence.basicDependencies.edge:\n",
    "            dep_relation = dep.dep\n",
    "            governor = sentence.token[dep.source - 1].word  # source token (index is 1-based)\n",
    "            dependent = sentence.token[dep.target - 1].word  # target token\n",
    "            print(f\"{governor} ({dep_relation}) -> {dependent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8f8058-3387-433a-a8a3-7fac4e3c507e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['West of House', 'You are standing in an open field west of a white house, with a boarded front door.', 'There is a small mailbox here.', '']\n",
      "OpenIE error\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "from jericho.util import clean\n",
    "\n",
    "def call_stanford_openie(sentence):\n",
    "    url = \"http://localhost:9000/\"\n",
    "    querystring = {\n",
    "        \"properties\": \"%7B%22annotators%22%3A%20%22openie%22%7D\",\n",
    "        \"pipelineLanguage\": \"en\"}\n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, data=sentence, params=querystring)\n",
    "        response = json.JSONDecoder().decode(response.text)\n",
    "    except:\n",
    "        print(\"OpenIE error\")\n",
    "        logging.debug('OpenIE Error: ' + sentence)\n",
    "        response = {\"sentences\": \"\"}\n",
    "    return response\n",
    "\n",
    "\n",
    "visible_state = \"\"\"West of House\n",
    "You are standing in an open field west of a white house, with a boarded front door.\n",
    "There is a small mailbox here.\n",
    "\"\"\"\n",
    "\n",
    "inventory_state = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "prev_action = None\n",
    "# obj is provided by Jericho api function .__identify_interactive_objects()\n",
    "objs = {'The small mailbox is closed.': [('mailbox', 'NOUN', 'LOC'), ('small', 'ADJ', 'LOC')], 'The door is closed.': [('front', 'ADJ', 'LOC'), ('door', 'NOUN', 'LOC')], 'The house is a beautiful colonial house which is painted white. It is clear that the owners must have been extremely wealthy.': [('house', 'NOUN', 'LOC'), ('white', 'NOUN', 'LOC')], 'all': [('all', 'NOUN', 'LOC')]}\n",
    "\n",
    "\n",
    "visible_state = visible_state.split('\\n')\n",
    "room = visible_state[0]\n",
    "print(visible_state)\n",
    "visible_state = clean(' '.join(visible_state[1:]))\n",
    "\n",
    "dirs = ['north', 'south', 'east', 'west', 'southeast', 'southwest', 'northeast', 'northwest', 'up', 'down']\n",
    "\n",
    "visible_state = str(visible_state)\n",
    "rules = []\n",
    "\n",
    "sents = call_stanford_openie(visible_state)['sentences']\n",
    "\n",
    "in_aliases = ['are in', 'are facing', 'are standing', 'are behind', 'are above', 'are below', 'are in front']\n",
    "\n",
    "in_rl = []\n",
    "in_flag = False\n",
    "\n",
    "for i, ov in enumerate(sents):\n",
    "    sent = ' '.join([a['word'] for a in ov['tokens']])\n",
    "    triple = ov['openie']\n",
    "    for d in dirs:\n",
    "        if d in sent and i != 0:\n",
    "            rules.append((room, 'has', 'exit to ' + d))\n",
    "\n",
    "    for tr in triple:\n",
    "        h, r, t = tr['subject'].lower(), tr['relation'].lower(), tr['object'].lower()\n",
    "        if h == 'you':\n",
    "            for rp in in_aliases:\n",
    "                if fuzz.token_set_ratio(r, rp) > 80:\n",
    "                    r = \"in\"\n",
    "                    in_rl.append((h, r, t))\n",
    "                    in_flag = True\n",
    "                    break\n",
    "\n",
    "        if h == 'it':\n",
    "            break\n",
    "        if not in_flag:\n",
    "            rules.append((h, r, t))\n",
    "\n",
    "if in_flag:\n",
    "    cur_t = in_rl[0]\n",
    "    for h, r, t in in_rl:\n",
    "        if set(cur_t[2].split()).issubset(set(t.split())):\n",
    "            cur_t = h, r, t\n",
    "    rules.append(cur_t)\n",
    "    room = cur_t[2]\n",
    "\n",
    "try:\n",
    "    items = inventory_state.split(':')[1].split('\\n')[1:]\n",
    "    for item in items:\n",
    "        rules.append(('you', 'have', str(' ' .join(item.split()[1:]))))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if prev_action is not None:\n",
    "    pass\n",
    "    # for d in dirs:\n",
    "    #     if d in prev_action and self.room != \"\":\n",
    "    #         rules.append((prev_room, d + ' of', room))\n",
    "    #         if prev_room_subgraph is not None:\n",
    "    #             for ed in prev_room_subgraph.edges:\n",
    "    #                 rules.append((ed[0], prev_room_subgraph[ed]['rel'], ed[1]))\n",
    "    #         break\n",
    "\n",
    "for o in objs:\n",
    "    #if o != 'all':\n",
    "    rules.append((str(o), 'in', room))\n",
    "\n",
    "add_rules = rules\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba1653e-6c0e-45ca-917a-7f0ba026e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The small mailbox is closed.', 'in', 'West of House'), ('The door is closed.', 'in', 'West of House'), ('The house is a beautiful colonial house which is painted white. It is clear that the owners must have been extremely wealthy.', 'in', 'West of House'), ('all', 'in', 'West of House')]\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(add_rules)\n",
    "print('*'*60)\n",
    "\n",
    "# for sent in sents:\n",
    "#     for dep in sent['basicDependencies']:\n",
    "#         print(dep)\n",
    "#         print('-' * 60)\n",
    "#     print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f8ab3-eb71-4219-9e50-efb5d90b61ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda (p2_gameagents_3.11)",
   "language": "python",
   "name": "p2_gameagents_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
